{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from kaggle_camera_model_id_lib.utils import PechkaBot, ImageList, NpzFolder, NCrops, TifFolderExFiles\n",
    "from kaggle_camera_model_id_lib.models import VggHead, StyleVggHead, IEEEfcn, ResNetFC, FatNet1,InceptionResNetV2\n",
    "from kaggle_camera_model_id_lib.utils import jpg_compress, equalize_v_hist, hsv_convert\n",
    "from kaggle_camera_model_id_lib.utils import scale_crop_pad, gamma_correction\n",
    "from kaggle_camera_model_id_lib.utils import patch_quality_dich, n_random_crops, n_pseudorandom_crops\n",
    "from kaggle_camera_model_id_lib.models import ResNetDense, ResNetDenseFC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = '/home/mephistopheies/storage2/data/camera-model-id/val/'\n",
    "test_path = '/home/mephistopheies/storage2/data/camera-model-id/raw/test/'\n",
    "model_path = '/home/mephistopheies/storage2/data/camera-model-id/models/ResNetDenseFC34/256_random_aug_kaggle_10_pretrained_zfc_flickr_noval_nocenter/checkpoint.tar'\n",
    "out_dir = '/home/mephistopheies/storage2/data/camera-model-id/submit/'\n",
    "model_type = 'ResNetDenseFC34_pretrained_zfc'\n",
    "n_classes = 10\n",
    "crop_size = 256\n",
    "step = 128\n",
    "num_workers = 1\n",
    "\n",
    "do_random_aug_kaggle = True\n",
    "p_random_aug_kaggle = 0.5\n",
    "do_hard_aug = False\n",
    "p_hard_aug = 0.5\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "scale_05 = lambda img: scale_crop_pad(img, 0.5)\n",
    "scale_08 = lambda img: scale_crop_pad(img, 0.8)\n",
    "scale_15 = lambda img: scale_crop_pad(img, 1.5)\n",
    "scale_20 = lambda img: scale_crop_pad(img, 2.0)\n",
    "gamma_08 = lambda img: gamma_correction(img, 0.8)\n",
    "gamma_12 = lambda img: gamma_correction(img, 1.2)\n",
    "jpg_70 = lambda img: jpg_compress(img, (70, 71))\n",
    "jpg_90 = lambda img: jpg_compress(img, (90, 91))\n",
    "augs = [scale_05, scale_08, scale_15, scale_20, gamma_08, gamma_12, jpg_70, jpg_90]\n",
    "\n",
    "blur = iaa.GaussianBlur(sigma=(0, 2))\n",
    "sharpen = iaa.Sharpen(alpha=(0, 1), lightness=(0.5, 2))\n",
    "emboss = iaa.Emboss(alpha=(0, 1), strength=(0, 2))\n",
    "contrast_normalization = iaa.ContrastNormalization(alpha=(0.7, 1.3))\n",
    "hard_aug = iaa.OneOf([blur, sharpen, emboss, contrast_normalization])\n",
    "sometimes = iaa.Sometimes(p_hard_aug, hard_aug)\n",
    "\n",
    "\n",
    "def random_aug_kaggle(img, p=0.5):\n",
    "    if np.random.rand() < p:\n",
    "        return random.choice(augs)(img)\n",
    "    return img\n",
    "\n",
    "def aug_train(img):\n",
    "    if min(img.size) > crop_center_size:\n",
    "        return random_flip(random_crop(center_crop(img)))\n",
    "    return random_flip(random_crop(img))\n",
    "\n",
    "def aug_optional(img):\n",
    "    if do_hard_aug:\n",
    "        img = Image.fromarray(sometimes.augment_image(np.array(img)))\n",
    "\n",
    "    if do_random_aug_kaggle:\n",
    "        img = random_aug_kaggle(img, p_random_aug_kaggle)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last state:\n",
      "  TLoss: 0.092695\n",
      "  TAcc:  0.9690\n",
      "  VLoss: 0.000000\n",
      "  VAcc:  0.0000\n"
     ]
    }
   ],
   "source": [
    "model_factory = {\n",
    "    'Vgg19Head_E_2b_bn': lambda n_classes: VggHead(num_classes=n_classes, vgg_key='E_2b', load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_3b_bn': lambda n_classes: VggHead(num_classes=n_classes, vgg_key='E_3b', load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='E', batch_norm=True),\n",
    "    'Vgg11Head_A_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=True),\n",
    "    'Vgg11Head_A': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=False),\n",
    "    'StyleVggHead_bn': lambda n_classes: StyleVggHead(num_classes=n_classes, load_vgg_bn=True),\n",
    "    'IEEEfcn': lambda n_classes: IEEEfcn(n_classes),\n",
    "    'resnet18fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet='resnet18'),\n",
    "    'resnet18fc': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet=None),\n",
    "    'resnet18X_pretrained': lambda n_classes: ResNetX(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet='resnet18'),\n",
    "    'InceptionResNetV2fc_5_10_4': lambda n_classes: InceptionResNetV2fc(\n",
    "        num_classes=n_classes, nun_block35=5, num_block17=10, num_block8=4),\n",
    "    'InceptionResNetV2fcSmall_5_10': lambda n_classes: InceptionResNetV2fcSmall(\n",
    "        num_classes=n_classes, nun_block35=5, num_block17=10),\n",
    "    'resnet34fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34'),\n",
    "    'resnet34fc_pretrained_maxpool': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', pool_type='max'),\n",
    "    'resnet50fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet50'),\n",
    "    'FatNet1': lambda n_classes: FatNet1(n_classes),\n",
    "    'resnet34X_pretrained_maxpool': lambda n_classes: ResNetX(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', pool_type='max'),\n",
    "    'resnet50X_pretrained_maxpool': lambda n_classes: ResNetX(\n",
    "        models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet50', pool_type='max'),\n",
    "    'InceptionResNetV2': lambda n_classes: InceptionResNetV2(num_classes=n_classes),\n",
    "    'ResNetDense34_pretrained': lambda n_classes: ResNetDense(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34'),\n",
    "    'ResNetDenseFC34_pretrained': lambda n_classes: ResNetDenseFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', \n",
    "        zero_first_center=False),\n",
    "    'ResNetDenseFC34_pretrained_zfc': lambda n_classes: ResNetDenseFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', \n",
    "        zero_first_center=True)\n",
    "}\n",
    "\n",
    "model = model_factory[model_type](n_classes)\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "loss_train = checkpoint['loss_train']\n",
    "acc_train = checkpoint['acc_train']\n",
    "loss_val = checkpoint['loss_val']\n",
    "acc_val = checkpoint['acc_val']\n",
    "class_to_idx = checkpoint['class_to_idx']\n",
    "idx2class = dict([(v, k) for (k, v) in class_to_idx.items()])\n",
    "print('Last state:\\n  TLoss: %0.6f\\n  TAcc:  %0.4f\\n  VLoss: %0.6f\\n  VAcc:  %0.4f' % \n",
    "    (loss_train[-1], acc_train[-1], loss_val[-1], acc_val[-1]))\n",
    "del(checkpoint)\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e33f7eca1a463d9317b23d2f1720c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "ds_test = TifFolderExFiles(\n",
    "    test_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "    ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(    \n",
    "    ds_test,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True)\n",
    "\n",
    "hooks = []\n",
    "avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "def extract_features(self, input, output):\n",
    "    self.last_input_values = avg_pool(input[0])\n",
    "\n",
    "hooks.append(model.project.register_forward_hook(extract_features))\n",
    "\n",
    "\n",
    "\n",
    "probs = []\n",
    "feats = []\n",
    "files = []\n",
    "classes = [v for (k, v) in sorted(idx2class.items(), key=lambda t: t[0])]\n",
    "\n",
    "for X, Y, files_tmp in tqdm_notebook(test_loader, total=int(len(ds_test.imgs)/batch_size)):\n",
    "    files_tmp = list(map(lambda s: os.path.basename(s), files_tmp))\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    log_p = model(X_var)\n",
    "    log_p = log_p.view(bs, ncrops, -1)\n",
    "    p = F.softmax(log_p, dim=2).mean(dim=1)\n",
    "    #p = p.prod(dim=1).pow(1/p.shape[1])    \n",
    "    f = model.project.last_input_values.view(bs, ncrops, -1).squeeze()\n",
    "        \n",
    "    files.append(files_tmp[0])\n",
    "    probs.append(p.data.cpu().numpy().squeeze())\n",
    "    feats.append(f.mean(dim=0).data.cpu().numpy())\n",
    "    \n",
    "    \n",
    "probs = np.array(probs)\n",
    "feats = np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/home/mephistopheies/storage2/data/camera-model-id/blending/rn34densefc_02feb.npz',\n",
    "         classes=classes,\n",
    "         files=files,\n",
    "         probs=probs,\n",
    "         feats=feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = np.load('/home/mephistopheies/storage2/data/camera-model-id/blending/rn34densefc_02feb.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

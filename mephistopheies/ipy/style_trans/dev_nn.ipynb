{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from kaggle_camera_model_id_lib.utils import PechkaBot, ImageList, NpzFolder, NCrops, TifFolder, MultiDataset\n",
    "from kaggle_camera_model_id_lib.models import VggHead, StyleVggHead, IEEEfcn, ResNetFC, ResNetX, FatNet1\n",
    "from kaggle_camera_model_id_lib.models import InceptionResNetV2fc, InceptionResNetV2fcSmall\n",
    "from kaggle_camera_model_id_lib.utils import jpg_compress, equalize_v_hist, hsv_convert\n",
    "from kaggle_camera_model_id_lib.utils import scale_crop_pad, gamma_correction\n",
    "from kaggle_camera_model_id_lib.utils import patch_quality_dich, n_random_crops, n_pseudorandom_crops\n",
    "from kaggle_camera_model_id_lib.models import DANet, ResNetFeatureExtractor, AvgFcClassifier, FCDiscriminator\n",
    "from kaggle_camera_model_id_lib.models import AvgClassifier, InceptionResNetV2, ResNetDense, ResNetDenseFC\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import pretrainedmodels as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_center_size = 1520\n",
    "crop_size = 256\n",
    "random_crop = transforms.RandomCrop(crop_size)\n",
    "center_crop = transforms.CenterCrop(crop_center_size)\n",
    "rvf = transforms.RandomVerticalFlip()\n",
    "rhf = transforms.RandomHorizontalFlip()\n",
    "random_flip = lambda img: rvf(rhf(img))\n",
    "\n",
    "n_crops_search_train = 36\n",
    "n_crops_train = 9\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list_path = '/home/mephistopheies/storage2/data/camera-model-id/train.tsv'\n",
    "val_path = '/home/mephistopheies/storage2/data/camera-model-id/val/'\n",
    "test_path = '/home/mephistopheies/storage2/data/camera-model-id/raw/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_var = Variable(torch.from_numpy(np.random.uniform(-1, 1, size=(5, 3, 64, 64)).astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1.2, 1.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "\n",
    "model = ResNetDenseFC(\n",
    "            models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', \n",
    "            zero_first_center=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64, 16, 16])\n",
      "torch.Size([5, 128, 8, 8])\n",
      "torch.Size([5, 256, 4, 4])\n",
      "torch.Size([5, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = model.conv1(X_var)\n",
    "x = model.bn1(x)\n",
    "x = model.relu(x)\n",
    "x = model.maxpool(x)\n",
    "\n",
    "x1 = model.layer1(x)\n",
    "x2 = model.layer2(x1)\n",
    "x3 = model.layer3(x2)\n",
    "x4 = model.layer4(x3)\n",
    "\n",
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(x3.shape)\n",
    "print(x4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64, 2, 2])\n",
      "torch.Size([5, 128, 2, 2])\n",
      "torch.Size([5, 256, 2, 2])\n",
      "torch.Size([5, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x1 = model.maxpool8(x1)\n",
    "x2 = model.maxpool4(x2)\n",
    "x3 = model.maxpool2(x3)\n",
    "\n",
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(x3.shape)\n",
    "print(x4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NCrops(np.random.uniform(0, 1, size=(512, 512, 3)), 64, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

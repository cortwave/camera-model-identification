{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "from kaggle_camera_model_id_lib.utils import PechkaBot, ImageList, NpzFolder, NCrops, TifFolder, MultiDataset\n",
    "from kaggle_camera_model_id_lib.models import VggHead, StyleVggHead, IEEEfcn, ResNetFC, ResNetX, FatNet1\n",
    "from kaggle_camera_model_id_lib.models import InceptionResNetV2fc, InceptionResNetV2fcSmall\n",
    "from kaggle_camera_model_id_lib.utils import jpg_compress, equalize_v_hist, hsv_convert\n",
    "from kaggle_camera_model_id_lib.utils import scale_crop_pad, gamma_correction\n",
    "from kaggle_camera_model_id_lib.utils import patch_quality_dich, n_random_crops, n_pseudorandom_crops\n",
    "from kaggle_camera_model_id_lib.models import DANet, ResNetFeatureExtractor, AvgFcClassifier, FCDiscriminator\n",
    "from kaggle_camera_model_id_lib.models import AvgClassifier\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_center_size = 1520\n",
    "crop_size = 256\n",
    "random_crop = transforms.RandomCrop(crop_size)\n",
    "center_crop = transforms.CenterCrop(crop_center_size)\n",
    "rvf = transforms.RandomVerticalFlip()\n",
    "rhf = transforms.RandomHorizontalFlip()\n",
    "random_flip = lambda img: rvf(rhf(img))\n",
    "\n",
    "n_crops_search_train = 36\n",
    "n_crops_train = 9\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list_path = '/home/mephistopheies/storage2/data/camera-model-id/train.tsv'\n",
    "val_path = '/home/mephistopheies/storage2/data/camera-model-id/val/'\n",
    "test_path = '/home/mephistopheies/storage2/data/camera-model-id/raw/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_var = Variable(torch.from_numpy(np.random.uniform(-1, 1, size=(5, 3, 256, 256)).astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_classes = 10 \n",
    "\n",
    "# model_fe = ResNetFeatureExtractor(models.resnet.BasicBlock, [3, 4, 6, 3], \n",
    "#                                   load_resnet='resnet34')\n",
    "# model_c = AvgFcClassifier(n_classes)\n",
    "# model_d = FCDiscriminator()\n",
    "\n",
    "# model = DANet(model_fe, model_d, model_c)\n",
    "\n",
    "# print(model(X_var, mode='c'))\n",
    "# print(model(X_var, mode='d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/devnag/pytorch-generative-adversarial-networks/blob/master/gan_pytorch.py\n",
    "- https://github.com/znxlwm/pytorch-generative-model-collections\n",
    "- https://github.com/pytorch/examples/blob/master/dcgan/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step_crop_val = 256\n",
    "\n",
    "# ds_val = NpzFolder(\n",
    "#     val_path,\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.Lambda(lambda img: NCrops(img, crop_size=crop_size, step=step_crop_val)),\n",
    "#         transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(Image.fromarray(crop)))\n",
    "#                                                      for crop in crops]))\n",
    "#     ]),\n",
    "#     target_transform=transforms.Compose([\n",
    "#         transforms.Lambda(lambda y: [y]*int(np.floor(1 + (512 - crop_size)/step_crop_val))**2),\n",
    "#         transforms.Lambda(lambda ylist: torch.LongTensor(ylist))\n",
    "#     ]))\n",
    "# val_loader = torch.utils.data.DataLoader(    \n",
    "#     ds_val,\n",
    "#     batch_size=10, \n",
    "#     shuffle=False,\n",
    "#     num_workers=1, \n",
    "#     pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_factory = {\n",
    "    'resnet34_fe': lambda: ResNetFeatureExtractor(models.resnet.BasicBlock, [3, 4, 6, 3], load_resnet='resnet34'),\n",
    "    'AvgFcClassifier': lambda n_classes: AvgFcClassifier(n_classes),\n",
    "    'FCDiscriminator': lambda: FCDiscriminator(),\n",
    "    'AvgClassifier512': lambda n_classes: AvgClassifier(n_classes, 512)\n",
    "}\n",
    "\n",
    "model = DANet(\n",
    "        model_factory['resnet34_fe'](),\n",
    "        model_factory['FCDiscriminator'](),\n",
    "        model_factory['AvgClassifier512'](10)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = model.classifier.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('/home/mephistopheies/storage2/data/camera-model-id/models/resnet34fc/256_pretrained_random_aug_kaggle_10/best_model.tar')\n",
    "d = checkpoint['model']\n",
    "del(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion_dg = nn.BCELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     ds_val_c = NpzFolder(\n",
    "#         val_path,\n",
    "#         transform=transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: NCrops(img, crop_size=crop_size, step=step_crop_val)),\n",
    "#             transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(aug_optional_val(Image.fromarray(crop)))) \n",
    "#                                                          for crop in crops]))\n",
    "#         ]),\n",
    "#         target_transform=transforms.Compose([\n",
    "#             transforms.Lambda(lambda y: [y]*int(np.floor(1 + (512 - crop_size)/step_crop_val))**2),\n",
    "#             transforms.Lambda(lambda ylist: torch.LongTensor(ylist))\n",
    "#         ]))\n",
    "#     val_loader_c = torch.utils.data.DataLoader(    \n",
    "#         ds_val_c,\n",
    "#         batch_size=batch_size_val_c, \n",
    "#         shuffle=False,\n",
    "#         num_workers=workers, \n",
    "#         pin_memory=True)\n",
    "#     log('val_loader_c.size: %i' % len(val_loader_c.dataset.imgs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_size = 256\n",
    "step_crop_val = 128\n",
    "batch_size_train_d = 7\n",
    "workers = 1\n",
    "\n",
    "\n",
    "ds_train_d_test = TifFolder(\n",
    "    test_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step_crop_val)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "    ]),\n",
    "    target_transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda y: [0]*int(np.floor(1 + (512 - crop_size)/step_crop_val))**2),\n",
    "        transforms.Lambda(lambda ylist: torch.FloatTensor(ylist))\n",
    "    ]))\n",
    "train_loader_d_test = torch.utils.data.DataLoader(    \n",
    "    ds_train_d_test,\n",
    "    batch_size=batch_size_train_d, \n",
    "    shuffle=False,\n",
    "    num_workers=workers, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "ds_train_d_val = NpzFolder(\n",
    "    val_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(img, crop_size=crop_size, step=step_crop_val)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(Image.fromarray(crop)))\n",
    "                                                     for crop in crops]))\n",
    "    ]),\n",
    "    target_transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda y: [1]*int(np.floor(1 + (512 - crop_size)/step_crop_val))**2),\n",
    "        transforms.Lambda(lambda ylist: torch.FloatTensor(ylist))\n",
    "    ]))\n",
    "train_loader_d_val = torch.utils.data.DataLoader(    \n",
    "    ds_train_d_val,\n",
    "    batch_size=batch_size_train_d, \n",
    "    shuffle=False,\n",
    "    num_workers=workers, \n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader_d_test_chain = it.chain(*[train_loader_d_test]*5)\n",
    "train_loader_d_val_chain = it.chain(*[train_loader_d_val]*5)\n",
    "\n",
    "X_test, Y_test = train_loader_d_test_chain.__next__()\n",
    "X_val, Y_val = train_loader_d_val_chain.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 9, 3, 256, 256])\n",
      "torch.Size([7, 9])\n",
      "torch.Size([7, 9, 3, 256, 256])\n",
      "torch.Size([7, 9])\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 3, 256, 256])\n",
      "torch.Size([63])\n",
      "torch.Size([63, 3, 256, 256])\n",
      "torch.Size([63])\n",
      "torch.Size([126, 3, 256, 256])\n",
      "torch.Size([126])\n"
     ]
    }
   ],
   "source": [
    "bs, ncrops, c, h, w = X_test.shape\n",
    "X_test = X_test.view(-1, c, h, w)\n",
    "Y_test = Y_test.view(ncrops*bs)\n",
    "X_val = X_val.view(-1, c, h, w)\n",
    "Y_val = Y_val.view(ncrops*bs)\n",
    "\n",
    "X = torch.cat([X_test, X_val], dim=0)\n",
    "Y = torch.cat([Y_test, Y_val], dim=0)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_var = Variable(X.cuda())\n",
    "Y_var = Variable(Y.cuda())\n",
    "\n",
    "p = model(X_var, mode='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = criterion_dg(p.squeeze(), Y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_path = '/home/mephistopheies/storage2/data/camera-model-id/train.tsv'\n",
    "train_list_pseudo_npz = '/home/mephistopheies/storage2/data/camera-model-id/pseudo_labels/phase1/'\n",
    "\n",
    "n_crops_train = 5\n",
    "batch_size_train = 7\n",
    "workers = 1\n",
    "\n",
    "def aug_train(img):\n",
    "    if min(img.size) > crop_center_size:\n",
    "        return random_flip(random_crop(center_crop(img)))\n",
    "    img_np = np.array(img)\n",
    "    if img_np.shape[0] < crop_center_size and img_np.shape[1] > crop_center_size:\n",
    "        n = np.random.randint(img_np.shape[1] - crop_center_size)\n",
    "        return random_flip(random_crop(Image.fromarray(img_np[:, n:(n + crop_center_size), :])))\n",
    "    if img_np.shape[1] < crop_center_size and img_np.shape[0] > crop_center_size:\n",
    "        n = np.random.randint(img_np.shape[0] - crop_center_size)\n",
    "        return random_flip(random_crop(Image.fromarray(img_np[n:(n + crop_center_size), :, :])))\n",
    "    return random_flip(random_crop(img))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: [\n",
    "        aug_train(img)\n",
    "        for i in range(n_crops_train)\n",
    "    ]),\n",
    "    transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "])\n",
    "\n",
    "ds_train = ImageList(\n",
    "    train_list_path,\n",
    "    transform=transform_train,\n",
    "    target_transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda y: [y]*n_crops_train),\n",
    "        transforms.Lambda(lambda ylist: torch.LongTensor(ylist))\n",
    "    ]))\n",
    "\n",
    "    \n",
    "ds_train_pseudo = NpzFolder(\n",
    "    train_list_pseudo_npz,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: [\n",
    "            aug_train(Image.fromarray(img))\n",
    "            for i in range(n_crops_train)\n",
    "        ]),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "    ]),\n",
    "    target_transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda y: [y]*n_crops_train),\n",
    "        transforms.Lambda(lambda ylist: torch.LongTensor(ylist))\n",
    "    ]))\n",
    "# ds_train = MultiDataset([ds_train, ds_train_pseudo])\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(    \n",
    "#     ds_train,\n",
    "#     batch_size=batch_size_train, \n",
    "#     shuffle=True,\n",
    "#     num_workers=workers, \n",
    "#     pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 256, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[111][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train_pseudo[111][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train_pseudo[111][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

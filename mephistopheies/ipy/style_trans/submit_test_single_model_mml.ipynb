{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from kaggle_camera_model_id_lib.utils import PechkaBot, ImageList, NpzFolder, NCrops, TifFolderExFiles\n",
    "from kaggle_camera_model_id_lib.models import VggHead, StyleVggHead, IEEEfcn, ResNetFC\n",
    "from kaggle_camera_model_id_lib.utils import jpg_compress, equalize_v_hist, hsv_convert\n",
    "from kaggle_camera_model_id_lib.utils import scale_crop_pad, gamma_correction\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = '/home/mephistopheies/storage2/data/camera-model-id/val/'\n",
    "test_path = '/home/mephistopheies/storage2/data/camera-model-id/raw/test/'\n",
    "model_path = '/home/mephistopheies/storage2/data/camera-model-id/models/resnet34fc/256_mml_pretrained_random_aug_kaggle_10/best_model.tar'\n",
    "out_dir = '/home/mephistopheies/storage2/data/camera-model-id/submit/'\n",
    "model_type = 'resnet34fc_pretrained'\n",
    "n_classes = 10\n",
    "crop_size = 256\n",
    "step = 128\n",
    "num_workers = 1\n",
    "\n",
    "do_random_aug_kaggle = True\n",
    "p_random_aug_kaggle = 1.0\n",
    "do_hard_aug = False\n",
    "p_hard_aug = 0.5\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "scale_05 = lambda img: scale_crop_pad(img, 0.5)\n",
    "scale_08 = lambda img: scale_crop_pad(img, 0.8)\n",
    "scale_15 = lambda img: scale_crop_pad(img, 1.5)\n",
    "scale_20 = lambda img: scale_crop_pad(img, 2.0)\n",
    "gamma_08 = lambda img: gamma_correction(img, 0.8)\n",
    "gamma_12 = lambda img: gamma_correction(img, 1.2)\n",
    "jpg_70 = lambda img: jpg_compress(img, (70, 71))\n",
    "jpg_90 = lambda img: jpg_compress(img, (90, 91))\n",
    "augs = [scale_05, scale_08, scale_15, scale_20, gamma_08, gamma_12, jpg_70, jpg_90]\n",
    "\n",
    "blur = iaa.GaussianBlur(sigma=(0, 2))\n",
    "sharpen = iaa.Sharpen(alpha=(0, 1), lightness=(0.5, 2))\n",
    "emboss = iaa.Emboss(alpha=(0, 1), strength=(0, 2))\n",
    "contrast_normalization = iaa.ContrastNormalization(alpha=(0.7, 1.3))\n",
    "hard_aug = iaa.OneOf([blur, sharpen, emboss, contrast_normalization])\n",
    "sometimes = iaa.Sometimes(p_hard_aug, hard_aug)\n",
    "\n",
    "\n",
    "def random_aug_kaggle(img, p=0.5):\n",
    "    if np.random.rand() < p:\n",
    "        return random.choice(augs)(img)\n",
    "    return img\n",
    "\n",
    "def aug_train(img):\n",
    "    if min(img.size) > crop_center_size:\n",
    "        return random_flip(random_crop(center_crop(img)))\n",
    "    return random_flip(random_crop(img))\n",
    "\n",
    "def aug_optional(img):\n",
    "    if do_hard_aug:\n",
    "        img = Image.fromarray(sometimes.augment_image(np.array(img)))\n",
    "\n",
    "    if do_random_aug_kaggle:\n",
    "        img = random_aug_kaggle(img, p_random_aug_kaggle)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last state:\n",
      "  TLoss: 0.049621\n",
      "  TAcc:  0.9733\n",
      "  VLoss: 0.246174\n",
      "  VAcc:  0.8966\n"
     ]
    }
   ],
   "source": [
    "model_factory = {\n",
    "    'Vgg19Head_E_2b_bn': lambda n_classes: VggHead(num_classes=n_classes, vgg_key='E_2b', load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_3b_bn': lambda n_classes: VggHead(num_classes=n_classes, vgg_key='E_3b', load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='E', batch_norm=True),\n",
    "    'Vgg11Head_A_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=True),\n",
    "    'Vgg11Head_A': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=False),\n",
    "    'StyleVggHead_bn': lambda n_classes: StyleVggHead(num_classes=n_classes, load_vgg_bn=True),\n",
    "    'IEEEfcn': lambda n_classes: IEEEfcn(n_classes),\n",
    "    'resnet18fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet='resnet18'),\n",
    "    'resnet18fc': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet=None),\n",
    "    'resnet18X_pretrained': lambda n_classes: ResNetX(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet='resnet18'),\n",
    "    'InceptionResNetV2fc_5_10_4': lambda n_classes: InceptionResNetV2fc(\n",
    "        num_classes=n_classes, nun_block35=5, num_block17=10, num_block8=4),\n",
    "    'InceptionResNetV2fcSmall_5_10': lambda n_classes: InceptionResNetV2fcSmall(\n",
    "        num_classes=n_classes, nun_block35=5, num_block17=10),\n",
    "    'resnet34fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34'),\n",
    "    'resnet50fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet50')\n",
    "}\n",
    "\n",
    "model = model_factory[model_type](n_classes)\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "loss_train = checkpoint['loss_train']\n",
    "acc_train = checkpoint['acc_train']\n",
    "loss_val = checkpoint['loss_val']\n",
    "acc_val = checkpoint['acc_val']\n",
    "class_to_idx = checkpoint['class_to_idx']\n",
    "idx2class = dict([(v, k) for (k, v) in class_to_idx.items()])\n",
    "print('Last state:\\n  TLoss: %0.6f\\n  TAcc:  %0.4f\\n  VLoss: %0.6f\\n  VAcc:  %0.4f' % \n",
    "    (loss_train[-1], acc_train[-1], loss_val[-1], acc_val[-1]))\n",
    "del(checkpoint)\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bd3caaae234f43a9832d8162e7c0eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.8945185029506684\n"
     ]
    }
   ],
   "source": [
    "batch_size = 15\n",
    "\n",
    "ds_val = NpzFolder(\n",
    "    val_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(img, crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(aug_optional(Image.fromarray(crop)))) \n",
    "                                                     for crop in crops]))\n",
    "    ]),\n",
    "    target_transform=transforms.Compose([\n",
    "            transforms.Lambda(lambda y: [y]*int(np.floor(1 + (512 - crop_size)/step))**2),\n",
    "            transforms.Lambda(lambda ylist: torch.LongTensor(ylist))\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(    \n",
    "    ds_val,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=1, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "acc_val_batch = 0\n",
    "for ix_batch, (X, Y) in tqdm_notebook(enumerate(val_loader), total=int(len(ds_val.imgs)/batch_size)):\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    Y = Y.view(ncrops*bs)\n",
    "\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    Y_var = Variable(Y.cuda(), volatile=True)\n",
    "\n",
    "    log_p = model(X_var)\n",
    "\n",
    "    acc_val_batch += ((log_p.max(1)[1] == Y_var).float().sum()/Y_var.shape[0]).data[0]\n",
    "\n",
    "acc_val_batch /= ix_batch + 1\n",
    "print(acc_val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c04087a7ea4a108bc020faaba5343d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.949333333333\n"
     ]
    }
   ],
   "source": [
    "batch_size = 15\n",
    "\n",
    "ds_val = NpzFolder(\n",
    "    val_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(img, crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(aug_optional(Image.fromarray(crop)))) \n",
    "                                                     for crop in crops]))\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(    \n",
    "    ds_val,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=1, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "# p_val = []\n",
    "acc_val_batch = 0\n",
    "for ix_batch, (X, Y) in tqdm_notebook(enumerate(val_loader), total=int(len(ds_val.imgs)/batch_size)):\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    Y_var = Variable(Y.cuda(), volatile=True)\n",
    "    log_p = model(X_var)\n",
    "    log_p = log_p.view(bs, ncrops, -1)\n",
    "    \n",
    "    p_amax = log_p.max(2)[1]\n",
    "    votes = pd.DataFrame(p_amax.cpu().data.numpy()).apply(lambda r: r.value_counts().index[0], axis=1).values\n",
    "    acc_val_batch += (Y_var.cpu().data.numpy() == votes).sum()/votes.shape[0]\n",
    "    \n",
    "\n",
    "#     p = F.softmax(log_p, dim=2)\n",
    "#     p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "#     acc_val_batch += ((p.max(1)[1] == Y_var).float().sum()/Y_var.shape[0]).data[0]\n",
    "#     p_val.append(p.cpu().data.numpy())\n",
    "\n",
    "# p_val = np.vstack(p_val)\n",
    "acc_val_batch /= ix_batch + 1\n",
    "print(acc_val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265006b93e6c4b76a8b1c076c39b0417"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_test = TifFolderExFiles(\n",
    "    test_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "    ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(    \n",
    "    ds_test,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True)\n",
    "\n",
    "res = []\n",
    "p_test = {}\n",
    "for X, Y, files in tqdm_notebook(test_loader, total=int(len(ds_test.imgs)/batch_size)):\n",
    "    files = list(map(lambda s: os.path.basename(s), files))\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    log_p = model(X_var)\n",
    "    log_p = log_p.view(bs, ncrops, -1)\n",
    "    \n",
    "    p_amax = log_p.max(2)[1]\n",
    "    votes = pd.DataFrame(p_amax.cpu().data.numpy()).apply(lambda r: r.value_counts().index[0], axis=1).values\n",
    "    res.extend(list(zip(files, [idx2class[ix] for ix in votes])))\n",
    "    \n",
    "#     p = F.softmax(log_p, dim=2) #.mean(dim=1)\n",
    "#     p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "#     ix_argmax = p.max(1)[1].cpu().data.numpy()\n",
    "#     res.extend(list(zip(files, [idx2class[ix] for ix in ix_argmax])))\n",
    "\n",
    "#     for ix in range(len(files)):\n",
    "#         p_test[files[ix]] = [(idx2class[i], x) for (i, x) in enumerate(p[ix, :].cpu().data.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scales = [(512 - 32*i, 64, 5) for i in range(5, 12)]\n",
    "# for t in scales:\n",
    "#     print(t)\n",
    "\n",
    "# res_scales = defaultdict(list)\n",
    "# for crop_size, step, batch_size in scales:\n",
    "#     ds_test = TifFolderExFiles(\n",
    "#         test_path,\n",
    "#         transform=transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "#             transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "#         ]))\n",
    "\n",
    "#     test_loader = torch.utils.data.DataLoader(    \n",
    "#         ds_test,\n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=False,\n",
    "#         num_workers=num_workers, \n",
    "#         pin_memory=True)\n",
    "    \n",
    "#     for X, Y, files in tqdm_notebook(test_loader, total=int(len(ds_test.imgs)/batch_size), desc=str(crop_size)):\n",
    "#         files = list(map(lambda s: os.path.basename(s), files))\n",
    "#         bs, ncrops, c, h, w = X.shape\n",
    "#         X = X.view(-1, c, h, w)\n",
    "#         X_var = Variable(X.cuda(), volatile=True)\n",
    "#         log_p = model(X_var)\n",
    "#         log_p = log_p.view(bs, ncrops, -1)\n",
    "#         p = F.softmax(log_p, dim=2)\n",
    "\n",
    "#         for ix, fname in enumerate(files):\n",
    "#             res_scales[fname].append(p[ix, :].cpu().data.numpy())\n",
    "            \n",
    "            \n",
    "# res = []\n",
    "# for fname, p in res_scales.items():\n",
    "#     p = np.vstack(p)\n",
    "#     if (p == 0).sum() > 0:\n",
    "#         p[p == 0] += 1e-16\n",
    "#     c_id = np.exp(np.log(p).mean(axis=0)).argmax()\n",
    "#     res.append((fname, idx2class[c_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crop_size = 256\n",
    "# step = 64\n",
    "# batch_size = 5\n",
    "\n",
    "# ds_test = TifFolderExFiles(\n",
    "#     test_path,\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "#         transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "#     ]))\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(    \n",
    "#     ds_test,\n",
    "#     batch_size=batch_size, \n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers, \n",
    "#     pin_memory=True)\n",
    "\n",
    "# res = []\n",
    "# p_test = {}\n",
    "# for X, Y, files in tqdm_notebook(test_loader, total=int(len(ds_test.imgs)/batch_size)):\n",
    "#     files = list(map(lambda s: os.path.basename(s), files))\n",
    "#     bs, ncrops, c, h, w = X.shape\n",
    "#     X = X.view(-1, c, h, w)\n",
    "#     X_var = Variable(X.cuda(), volatile=True)\n",
    "#     log_p = model(X_var)\n",
    "#     log_p = log_p.view(bs, ncrops, -1)\n",
    "#     p = F.softmax(log_p, dim=2) #.mean(dim=1)\n",
    "#     p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "#     ix_argmax = p.max(1)[1].cpu().data.numpy()\n",
    "#     res.extend(list(zip(files, [idx2class[ix] for ix in ix_argmax])))\n",
    "\n",
    "#     for ix in range(len(files)):\n",
    "#         p_test[files[ix]] = [(idx2class[i], x) for (i, x) in enumerate(p[ix, :].cpu().data.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, 'submit__unalt.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        if '_unalt' in fname:\n",
    "            f.write('%s,%s\\n' % (fname, c))\n",
    "        else:\n",
    "            f.write('%s,%s\\n' % (fname, 'no_class'))\n",
    "            \n",
    "            \n",
    "with open(os.path.join(out_dir, 'submit__manip.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        if '_manip' in fname:\n",
    "            f.write('%s,%s\\n' % (fname, c))\n",
    "        else:\n",
    "            f.write('%s,%s\\n' % (fname, 'no_class'))\n",
    "            \n",
    "            \n",
    "with open(os.path.join(out_dir, 'submit.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        f.write('%s,%s\\n' % (fname, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

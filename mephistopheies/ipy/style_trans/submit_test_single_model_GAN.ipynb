{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from kaggle_camera_model_id_lib.utils import PechkaBot, ImageList, NpzFolder, NCrops, TifFolder, TifFolderExFiles\n",
    "from kaggle_camera_model_id_lib.models import VggHead, StyleVggHead, IEEEfcn, ResNetFC, ResNetX, FatNet1\n",
    "from kaggle_camera_model_id_lib.models import InceptionResNetV2fc, InceptionResNetV2fcSmall\n",
    "from kaggle_camera_model_id_lib.utils import jpg_compress, equalize_v_hist, hsv_convert\n",
    "from kaggle_camera_model_id_lib.utils import scale_crop_pad, gamma_correction\n",
    "from kaggle_camera_model_id_lib.utils import patch_quality_dich, n_random_crops, n_pseudorandom_crops\n",
    "from kaggle_camera_model_id_lib.models import DANet, ResNetFeatureExtractor, AvgFcClassifier, FCDiscriminator\n",
    "from kaggle_camera_model_id_lib.models import AvgClassifier\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_path = '/home/mephistopheies/storage2/data/camera-model-id/val/'\n",
    "test_path = '/home/mephistopheies/storage2/data/camera-model-id/raw/test/'\n",
    "model_path = '/home/mephistopheies/storage2/data/camera-model-id/models/resnet34fc/gan/FCDiscriminator_AvgClassifier512/var2/checkpoint.tar'\n",
    "out_dir = '/home/mephistopheies/storage2/data/camera-model-id/submit/'\n",
    "\n",
    "model_type_fe = 'resnet34_fe'\n",
    "model_type_d = 'FCDiscriminator'\n",
    "model_type_c = 'AvgClassifier512'\n",
    "\n",
    "n_classes = 10\n",
    "crop_size = 256\n",
    "step = 128\n",
    "num_workers = 1\n",
    "\n",
    "do_random_aug_kaggle = True\n",
    "p_random_aug_kaggle = 0.5\n",
    "do_hard_aug = False\n",
    "p_hard_aug = 0.5\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "scale_05 = lambda img: scale_crop_pad(img, 0.5)\n",
    "scale_08 = lambda img: scale_crop_pad(img, 0.8)\n",
    "scale_15 = lambda img: scale_crop_pad(img, 1.5)\n",
    "scale_20 = lambda img: scale_crop_pad(img, 2.0)\n",
    "gamma_08 = lambda img: gamma_correction(img, 0.8)\n",
    "gamma_12 = lambda img: gamma_correction(img, 1.2)\n",
    "jpg_70 = lambda img: jpg_compress(img, (70, 71))\n",
    "jpg_90 = lambda img: jpg_compress(img, (90, 91))\n",
    "augs = [scale_05, scale_08, scale_15, scale_20, gamma_08, gamma_12, jpg_70, jpg_90]\n",
    "\n",
    "blur = iaa.GaussianBlur(sigma=(0, 2))\n",
    "sharpen = iaa.Sharpen(alpha=(0, 1), lightness=(0.5, 2))\n",
    "emboss = iaa.Emboss(alpha=(0, 1), strength=(0, 2))\n",
    "contrast_normalization = iaa.ContrastNormalization(alpha=(0.7, 1.3))\n",
    "hard_aug = iaa.OneOf([blur, sharpen, emboss, contrast_normalization])\n",
    "sometimes = iaa.Sometimes(p_hard_aug, hard_aug)\n",
    "\n",
    "\n",
    "def random_aug_kaggle(img, p=0.5):\n",
    "    if np.random.rand() < p:\n",
    "        return random.choice(augs)(img)\n",
    "    return img\n",
    "\n",
    "def aug_train(img):\n",
    "    if min(img.size) > crop_center_size:\n",
    "        return random_flip(random_crop(center_crop(img)))\n",
    "    return random_flip(random_crop(img))\n",
    "\n",
    "def aug_optional(img):\n",
    "    if do_hard_aug:\n",
    "        img = Image.fromarray(sometimes.augment_image(np.array(img)))\n",
    "\n",
    "    if do_random_aug_kaggle:\n",
    "        img = random_aug_kaggle(img, p_random_aug_kaggle)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  acc_train_c: 0.995298\n",
      "\n",
      "  acc_train_d: 0.995153\n",
      "\n",
      "  acc_train_g: 0.998333\n",
      "\n",
      "  acc_val: 0.974519\n",
      "\n",
      "  loss_train_c: 0.014187\n",
      "\n",
      "  loss_train_d: 0.014571\n",
      "\n",
      "  loss_train_g: 0.007592\n",
      "\n",
      "  loss_val: 0.086873\n",
      "\n",
      "  time: 536.591299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_factory = {\n",
    "    'resnet34_fe': lambda: ResNetFeatureExtractor(models.resnet.BasicBlock, [3, 4, 6, 3], load_resnet='resnet34'),\n",
    "    'AvgFcClassifier': lambda n_classes: AvgFcClassifier(n_classes),\n",
    "    'FCDiscriminator': lambda: FCDiscriminator(),\n",
    "    'AvgClassifier512': lambda n_classes: AvgClassifier(n_classes, 512)\n",
    "}\n",
    "\n",
    "model = DANet(\n",
    "        model_factory[model_type_fe](),\n",
    "        model_factory[model_type_d](),\n",
    "        model_factory[model_type_c](n_classes))\n",
    "\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "state = checkpoint['model']\n",
    "model.load_state_dict(state)\n",
    "class_to_idx = checkpoint['class_to_idx']\n",
    "idx2class = dict([(v, k) for (k, v) in class_to_idx.items()])\n",
    "epoch_log = checkpoint['trainin_log']\n",
    "for k, v in sorted(epoch_log[-1].items(), key=lambda t: t[0]):\n",
    "    print('  %s: %0.6f\\n' % (k, v))\n",
    "del(checkpoint)\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6fe629622b4f9b8cd53dbc19adec91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.9758024575312932\n"
     ]
    }
   ],
   "source": [
    "batch_size = 15\n",
    "\n",
    "ds_val = NpzFolder(\n",
    "    val_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(img, crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(aug_optional(Image.fromarray(crop)))) \n",
    "                                                     for crop in crops]))\n",
    "    ]),\n",
    "    target_transform=transforms.Compose([\n",
    "            transforms.Lambda(lambda y: [y]*int(np.floor(1 + (512 - crop_size)/step))**2),\n",
    "            transforms.Lambda(lambda ylist: torch.LongTensor(ylist))\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(    \n",
    "    ds_val,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=1, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "acc_val_batch = 0\n",
    "for ix_batch, (X, Y) in tqdm_notebook(enumerate(val_loader), total=int(len(ds_val.imgs)/batch_size)):\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    Y = Y.view(ncrops*bs)\n",
    "\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    Y_var = Variable(Y.cuda(), volatile=True)\n",
    "\n",
    "    log_p = model(X_var, mode='c')\n",
    "\n",
    "    acc_val_batch += ((log_p.max(1)[1] == Y_var).float().sum()/Y_var.shape[0]).data[0]\n",
    "\n",
    "acc_val_batch /= ix_batch + 1\n",
    "print(acc_val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db0072e5bea45c4aec9fa191492a7c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.991555559237798\n"
     ]
    }
   ],
   "source": [
    "batch_size = 15\n",
    "\n",
    "ds_val = NpzFolder(\n",
    "    val_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(img, crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(aug_optional(Image.fromarray(crop)))) \n",
    "                                                     for crop in crops]))\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(    \n",
    "    ds_val,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=1, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "p_val = []\n",
    "acc_val_batch = 0\n",
    "for ix_batch, (X, Y) in tqdm_notebook(enumerate(val_loader), total=int(len(ds_val.imgs)/batch_size)):\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    Y_var = Variable(Y.cuda(), volatile=True)\n",
    "    log_p = model(X_var, mode='c')\n",
    "    log_p = log_p.view(bs, ncrops, -1)\n",
    "    p = F.softmax(log_p, dim=2)\n",
    "    p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "    acc_val_batch += ((p.max(1)[1] == Y_var).float().sum()/Y_var.shape[0]).data[0]\n",
    "    p_val.append(p.cpu().data.numpy())\n",
    "\n",
    "p_val = np.vstack(p_val)\n",
    "acc_val_batch /= ix_batch + 1\n",
    "print(acc_val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6c3eef23ed4921915121992fefe0f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "ds_test = TifFolderExFiles(\n",
    "    test_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "    ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(    \n",
    "    ds_test,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True)\n",
    "\n",
    "res = []\n",
    "p_test = {}\n",
    "for X, Y, files in tqdm_notebook(test_loader, total=int(len(ds_test.imgs)/batch_size)):\n",
    "    files = list(map(lambda s: os.path.basename(s), files))\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    log_p = model(X_var, mode='c')\n",
    "    log_p = log_p.view(bs, ncrops, -1)\n",
    "    p = F.softmax(log_p, dim=2) #.mean(dim=1)\n",
    "    p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "    ix_argmax = p.max(1)[1].cpu().data.numpy()\n",
    "    res.extend(list(zip(files, [idx2class[ix] for ix in ix_argmax])))\n",
    "\n",
    "    for ix in range(len(files)):\n",
    "        p_test[files[ix]] = [(idx2class[i], x) for (i, x) in enumerate(p[ix, :].cpu().data.numpy())]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Motorola-X              347\n",
       "Motorola-Droid-Maxx     298\n",
       "iPhone-6                281\n",
       "LG-Nexus-5x             274\n",
       "Samsung-Galaxy-Note3    268\n",
       "Samsung-Galaxy-S4       266\n",
       "HTC-1-M7                265\n",
       "Motorola-Nexus-6        263\n",
       "Sony-NEX-7              224\n",
       "iPhone-4s               154\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([v for (k, v) in res]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, 'submit__unalt.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        if '_unalt' in fname:\n",
    "            f.write('%s,%s\\n' % (fname, c))\n",
    "        else:\n",
    "            f.write('%s,%s\\n' % (fname, 'no_class'))\n",
    "            \n",
    "            \n",
    "with open(os.path.join(out_dir, 'submit__manip.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        if '_manip' in fname:\n",
    "            f.write('%s,%s\\n' % (fname, c))\n",
    "        else:\n",
    "            f.write('%s,%s\\n' % (fname, 'no_class'))\n",
    "            \n",
    "            \n",
    "with open(os.path.join(out_dir, 'submit.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        f.write('%s,%s\\n' % (fname, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

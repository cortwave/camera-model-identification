{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from kaggle_camera_model_id_lib.utils import PechkaBot, ImageList, NpzFolder, NCrops, TifFolderExFiles\n",
    "from kaggle_camera_model_id_lib.models import VggHead, StyleVggHead, IEEEfcn, ResNetFC, FatNet1,InceptionResNetV2\n",
    "from kaggle_camera_model_id_lib.utils import jpg_compress, equalize_v_hist, hsv_convert\n",
    "from kaggle_camera_model_id_lib.utils import scale_crop_pad, gamma_correction\n",
    "from kaggle_camera_model_id_lib.utils import patch_quality_dich, n_random_crops, n_pseudorandom_crops\n",
    "from kaggle_camera_model_id_lib.models import ResNetDense, ResNetDenseFC\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_path = '/home/mephistopheies/storage2/data/camera-model-id/val/'\n",
    "test_path = '/home/mephistopheies/storage2/data/camera-model-id/raw/test/'\n",
    "model_path = '/home/mephistopheies/storage2/data/camera-model-id/models/ResNetDenseFC34/256_random_aug_kaggle_10_pretrained_zfc_flickr_noval_nocenter/pseudo_phase_1/checkpoint.tar'\n",
    "out_dir = '/home/mephistopheies/storage2/data/camera-model-id/submit/'\n",
    "model_type = 'ResNetDenseFC34_pretrained_zfc'\n",
    "n_classes = 10\n",
    "crop_size = 256\n",
    "step = 128\n",
    "num_workers = 1\n",
    "\n",
    "do_random_aug_kaggle = True\n",
    "p_random_aug_kaggle = 0.5\n",
    "do_hard_aug = False\n",
    "p_hard_aug = 0.5\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "scale_05 = lambda img: scale_crop_pad(img, 0.5)\n",
    "scale_08 = lambda img: scale_crop_pad(img, 0.8)\n",
    "scale_15 = lambda img: scale_crop_pad(img, 1.5)\n",
    "scale_20 = lambda img: scale_crop_pad(img, 2.0)\n",
    "gamma_08 = lambda img: gamma_correction(img, 0.8)\n",
    "gamma_12 = lambda img: gamma_correction(img, 1.2)\n",
    "jpg_70 = lambda img: jpg_compress(img, (70, 71))\n",
    "jpg_90 = lambda img: jpg_compress(img, (90, 91))\n",
    "augs = [scale_05, scale_08, scale_15, scale_20, gamma_08, gamma_12, jpg_70, jpg_90]\n",
    "\n",
    "blur = iaa.GaussianBlur(sigma=(0, 2))\n",
    "sharpen = iaa.Sharpen(alpha=(0, 1), lightness=(0.5, 2))\n",
    "emboss = iaa.Emboss(alpha=(0, 1), strength=(0, 2))\n",
    "contrast_normalization = iaa.ContrastNormalization(alpha=(0.7, 1.3))\n",
    "hard_aug = iaa.OneOf([blur, sharpen, emboss, contrast_normalization])\n",
    "sometimes = iaa.Sometimes(p_hard_aug, hard_aug)\n",
    "\n",
    "\n",
    "def random_aug_kaggle(img, p=0.5):\n",
    "    if np.random.rand() < p:\n",
    "        return random.choice(augs)(img)\n",
    "    return img\n",
    "\n",
    "def aug_train(img):\n",
    "    if min(img.size) > crop_center_size:\n",
    "        return random_flip(random_crop(center_crop(img)))\n",
    "    return random_flip(random_crop(img))\n",
    "\n",
    "def aug_optional(img):\n",
    "    if do_hard_aug:\n",
    "        img = Image.fromarray(sometimes.augment_image(np.array(img)))\n",
    "\n",
    "    if do_random_aug_kaggle:\n",
    "        img = random_aug_kaggle(img, p_random_aug_kaggle)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last state:\n",
      "  TLoss: 0.031235\n",
      "  TAcc:  0.9896\n",
      "  VLoss: 0.000000\n",
      "  VAcc:  0.0000\n"
     ]
    }
   ],
   "source": [
    "model_factory = {\n",
    "    'Vgg19Head_E_2b_bn': lambda n_classes: VggHead(num_classes=n_classes, vgg_key='E_2b', load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_3b_bn': lambda n_classes: VggHead(num_classes=n_classes, vgg_key='E_3b', load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='E', batch_norm=True),\n",
    "    'Vgg11Head_A_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=True),\n",
    "    'Vgg11Head_A': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=False),\n",
    "    'StyleVggHead_bn': lambda n_classes: StyleVggHead(num_classes=n_classes, load_vgg_bn=True),\n",
    "    'IEEEfcn': lambda n_classes: IEEEfcn(n_classes),\n",
    "    'resnet18fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet='resnet18'),\n",
    "    'resnet18fc': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet=None),\n",
    "    'resnet18X_pretrained': lambda n_classes: ResNetX(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet='resnet18'),\n",
    "    'InceptionResNetV2fc_5_10_4': lambda n_classes: InceptionResNetV2fc(\n",
    "        num_classes=n_classes, nun_block35=5, num_block17=10, num_block8=4),\n",
    "    'InceptionResNetV2fcSmall_5_10': lambda n_classes: InceptionResNetV2fcSmall(\n",
    "        num_classes=n_classes, nun_block35=5, num_block17=10),\n",
    "    'resnet34fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34'),\n",
    "    'resnet34fc_pretrained_maxpool': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', pool_type='max'),\n",
    "    'resnet50fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet50'),\n",
    "    'FatNet1': lambda n_classes: FatNet1(n_classes),\n",
    "    'resnet34X_pretrained_maxpool': lambda n_classes: ResNetX(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', pool_type='max'),\n",
    "    'resnet50X_pretrained_maxpool': lambda n_classes: ResNetX(\n",
    "        models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet50', pool_type='max'),\n",
    "    'InceptionResNetV2': lambda n_classes: InceptionResNetV2(num_classes=n_classes),\n",
    "    'ResNetDense34_pretrained': lambda n_classes: ResNetDense(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34'),\n",
    "    'ResNetDenseFC34_pretrained': lambda n_classes: ResNetDenseFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', \n",
    "        zero_first_center=False),\n",
    "    'ResNetDenseFC34_pretrained_zfc': lambda n_classes: ResNetDenseFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34', \n",
    "        zero_first_center=True)\n",
    "}\n",
    "\n",
    "model = model_factory[model_type](n_classes)\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "loss_train = checkpoint['loss_train']\n",
    "acc_train = checkpoint['acc_train']\n",
    "loss_val = checkpoint['loss_val']\n",
    "acc_val = checkpoint['acc_val']\n",
    "class_to_idx = checkpoint['class_to_idx']\n",
    "idx2class = dict([(v, k) for (k, v) in class_to_idx.items()])\n",
    "print('Last state:\\n  TLoss: %0.6f\\n  TAcc:  %0.4f\\n  VLoss: %0.6f\\n  VAcc:  %0.4f' % \n",
    "    (loss_train[-1], acc_train[-1], loss_val[-1], acc_val[-1]))\n",
    "del(checkpoint)\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3cdca3b7eb46059db1e704baee61d9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.8707692307692307 0.890243902439 0.850931677019\n"
     ]
    }
   ],
   "source": [
    "holdout_path = '/home/mephistopheies/projects/kaggle/camera-model-identification/validation/'\n",
    "\n",
    "y_map = pd.read_csv(os.path.join(holdout_path, 'external_validation.csv')).set_index('fname')['camera'].to_dict()\n",
    "g_map = pd.read_csv(os.path.join(holdout_path, 'external_validation.csv')).set_index('fname')['is_altered'].to_dict()\n",
    "holdout_files = glob(os.path.join(holdout_path, '*.jpg'))\n",
    "\n",
    "def loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "\n",
    "acc = 0\n",
    "acc_manip = 0\n",
    "acc_unalt = 0\n",
    "\n",
    "def predict_image(img):\n",
    "    crops = NCrops(img, crop_size=crop_size, step=step)\n",
    "    X = torch.stack([normalize(to_tensor(aug_optional(Image.fromarray(crop)))) for crop in crops])\n",
    "    X = X.view(1, 9, 3, 256, 256)\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    log_p = model(X_var)\n",
    "    log_p = log_p.view(bs, ncrops, -1)\n",
    "    p = F.softmax(log_p, dim=2)\n",
    "    p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "    \n",
    "    return p\n",
    "\n",
    "for fname in tqdm_notebook(holdout_files):\n",
    "    img = loader(fname)\n",
    "    \n",
    "    p0 = predict_image(np.array(img))\n",
    "    p90 = predict_image(np.array(img.rotate(90)))\n",
    "    p180 = predict_image(np.array(img.rotate(180)))\n",
    "    p270 = predict_image(np.array(img.rotate(270)))\n",
    "    \n",
    "    p = torch.stack([p0.squeeze(), p90.squeeze(), p180.squeeze(), p270.squeeze()]).max(dim=0)[0]\n",
    "    \n",
    "    \n",
    "    y_pred = idx2class[p.max(0)[1].data[0]]\n",
    "    y_true = y_map[os.path.basename(fname)]\n",
    "    \n",
    "    acc += float(y_pred == y_true)\n",
    "    if g_map[os.path.basename(fname)] == 0:\n",
    "        acc_unalt += float(y_pred == y_true)\n",
    "    else:\n",
    "        acc_manip += float(y_pred == y_true)\n",
    "        \n",
    "acc /= len(holdout_files)\n",
    "acc_unalt /= (np.array(list(g_map.values())) == 0).sum()\n",
    "acc_manip /= (np.array(list(g_map.values())) == 1).sum()\n",
    "\n",
    "print(acc, acc_unalt, acc_manip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6cbdd44b0140ef9bfc33acce327832"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.9853333363930384\n"
     ]
    }
   ],
   "source": [
    "batch_size = 15\n",
    "\n",
    "ds_val = NpzFolder(\n",
    "    val_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(img, crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(aug_optional(Image.fromarray(crop)))) \n",
    "                                                     for crop in crops]))\n",
    "    ]))\n",
    "val_loader = torch.utils.data.DataLoader(    \n",
    "    ds_val,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=1, \n",
    "    pin_memory=True)\n",
    "\n",
    "\n",
    "p_val = []\n",
    "acc_val_batch = 0\n",
    "for ix_batch, (X, Y) in tqdm_notebook(enumerate(val_loader), total=int(len(ds_val.imgs)/batch_size)):\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    Y_var = Variable(Y.cuda(), volatile=True)\n",
    "    log_p = model(X_var)\n",
    "    log_p = log_p.view(bs, ncrops, -1)\n",
    "    p = F.softmax(log_p, dim=2)\n",
    "    p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "    acc_val_batch += ((p.max(1)[1] == Y_var).float().sum()/Y_var.shape[0]).data[0]\n",
    "    p_val.append(p.cpu().data.numpy())\n",
    "\n",
    "p_val = np.vstack(p_val)\n",
    "acc_val_batch /= ix_batch + 1\n",
    "print(acc_val_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8196d3e73ca24acba6970003aab176a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "ds_test = TifFolderExFiles(\n",
    "    test_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "    ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(    \n",
    "    ds_test,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True)\n",
    "\n",
    "res = []\n",
    "p_test = {}\n",
    "for X, Y, files in tqdm_notebook(test_loader, total=int(len(ds_test.imgs)/batch_size)):\n",
    "    files = list(map(lambda s: os.path.basename(s), files))\n",
    "    bs, ncrops, c, h, w = X.shape\n",
    "    X = X.view(-1, c, h, w)\n",
    "    X_var = Variable(X.cuda(), volatile=True)\n",
    "    log_p = model(X_var)\n",
    "    log_p = log_p.view(bs, ncrops, -1)\n",
    "    p = F.softmax(log_p, dim=2) #.mean(dim=1)\n",
    "    p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "    ix_argmax = p.max(1)[1].cpu().data.numpy()\n",
    "    res.extend(list(zip(files, [idx2class[ix] for ix in ix_argmax])))\n",
    "\n",
    "    for ix in range(len(files)):\n",
    "        p_test[files[ix]] = [(idx2class[i], x) for (i, x) in enumerate(p[ix, :].cpu().data.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # scales = [(512 - 32*i, 64, int((512/(12 - i))/16) - 1) for i in range(5, 12)]\n",
    "# scales = [(512 - 32*i, 64, 5) for i in range(5, 12)]\n",
    "# for t in scales:\n",
    "#     print(t)\n",
    "\n",
    "# res_scales = defaultdict(list)\n",
    "# for crop_size, step, batch_size in scales:\n",
    "#     ds_test = TifFolderExFiles(\n",
    "#         test_path,\n",
    "#         transform=transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "#             transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "#         ]))\n",
    "\n",
    "#     test_loader = torch.utils.data.DataLoader(    \n",
    "#         ds_test,\n",
    "#         batch_size=batch_size, \n",
    "#         shuffle=False,\n",
    "#         num_workers=num_workers, \n",
    "#         pin_memory=True)\n",
    "    \n",
    "#     for X, Y, files in tqdm_notebook(test_loader, total=int(len(ds_test.imgs)/batch_size), desc=str(crop_size)):\n",
    "#         files = list(map(lambda s: os.path.basename(s), files))\n",
    "#         bs, ncrops, c, h, w = X.shape\n",
    "#         X = X.view(-1, c, h, w)\n",
    "#         X_var = Variable(X.cuda(), volatile=True)\n",
    "#         log_p = model(X_var)\n",
    "#         log_p = log_p.view(bs, ncrops, -1)\n",
    "#         p = F.softmax(log_p, dim=2)\n",
    "\n",
    "#         for ix, fname in enumerate(files):\n",
    "#             res_scales[fname].append(p[ix, :].cpu().data.numpy())\n",
    "            \n",
    "            \n",
    "# res = []\n",
    "# for fname, p in res_scales.items():\n",
    "#     p = np.vstack(p)\n",
    "#     if (p == 0).sum() > 0:\n",
    "#         p[p == 0] += 1e-16\n",
    "#     c_id = np.exp(np.log(p).mean(axis=0)).argmax()\n",
    "#     res.append((fname, idx2class[c_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crop_size = 256\n",
    "# step = 64\n",
    "# batch_size = 5\n",
    "\n",
    "# ds_test = TifFolderExFiles(\n",
    "#     test_path,\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "#         transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "#     ]))\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(    \n",
    "#     ds_test,\n",
    "#     batch_size=batch_size, \n",
    "#     shuffle=False,\n",
    "#     num_workers=num_workers, \n",
    "#     pin_memory=True)\n",
    "\n",
    "# res = []\n",
    "# p_test = {}\n",
    "# for X, Y, files in tqdm_notebook(test_loader, total=int(len(ds_test.imgs)/batch_size)):\n",
    "#     files = list(map(lambda s: os.path.basename(s), files))\n",
    "#     bs, ncrops, c, h, w = X.shape\n",
    "#     X = X.view(-1, c, h, w)\n",
    "#     X_var = Variable(X.cuda(), volatile=True)\n",
    "#     log_p = model(X_var)\n",
    "#     log_p = log_p.view(bs, ncrops, -1)\n",
    "#     p = F.softmax(log_p, dim=2) #.mean(dim=1)\n",
    "#     p = p.prod(dim=1).pow(1/p.shape[1])\n",
    "#     ix_argmax = p.max(1)[1].cpu().data.numpy()\n",
    "#     res.extend(list(zip(files, [idx2class[ix] for ix in ix_argmax])))\n",
    "\n",
    "#     for ix in range(len(files)):\n",
    "#         p_test[files[ix]] = [(idx2class[i], x) for (i, x) in enumerate(p[ix, :].cpu().data.numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_files = glob(os.path.join(test_path, '*/*.tif'))\n",
    "\n",
    "# res = []\n",
    "# for fname in tqdm_notebook(test_files):\n",
    "#     crops = sorted([(c, patch_quality_dich(c/255.0)**2) for c in NCrops(np.array(Image.open(fname)), 256, 64)], \n",
    "#                    key=lambda t: t[1], reverse=True)\n",
    "    \n",
    "#     n = sum([v for k, v in crops])\n",
    "#     crops = [(k, v/n) for k, v in crops]\n",
    "#     batch = torch.stack([normalize(to_tensor(k)) for (k, v) in crops])\n",
    "#     X_var = Variable(batch.cuda(), volatile=True)\n",
    "#     log_p = model(X_var)\n",
    "#     p = F.softmax(log_p, dim=1).cpu().data.numpy()\n",
    "#     q = np.array([v for k, v in crops])\n",
    "#     p = (p*q.reshape(-1, 1)).sum(axis=0)\n",
    "#     ix = p.argmax()\n",
    "#     res.append((os.path.basename(fname), idx2class[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, 'submit__unalt.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        if '_unalt' in fname:\n",
    "            f.write('%s,%s\\n' % (fname, c))\n",
    "        else:\n",
    "            f.write('%s,%s\\n' % (fname, 'no_class'))\n",
    "            \n",
    "            \n",
    "with open(os.path.join(out_dir, 'submit__manip.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        if '_manip' in fname:\n",
    "            f.write('%s,%s\\n' % (fname, c))\n",
    "        else:\n",
    "            f.write('%s,%s\\n' % (fname, 'no_class'))\n",
    "            \n",
    "            \n",
    "with open(os.path.join(out_dir, 'submit.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        f.write('%s,%s\\n' % (fname, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Samsung-Galaxy-Note3    336\n",
       "Motorola-Nexus-6        285\n",
       "iPhone-4s               278\n",
       "iPhone-6                271\n",
       "Samsung-Galaxy-S4       269\n",
       "Sony-NEX-7              267\n",
       "Motorola-Droid-Maxx     260\n",
       "HTC-1-M7                259\n",
       "Motorola-X              238\n",
       "LG-Nexus-5x             177\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([v for (k, v) in res]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2640 1320\n",
      "2640 1320\n",
      "1320 1320\n",
      "132\n",
      "2640\n"
     ]
    }
   ],
   "source": [
    "manip = [(f, c) for (f, c) in p_test.items() if '_manip' in f]\n",
    "print(len(p_test), len(manip))\n",
    "\n",
    "unalt = [(f, c) for (f, c) in p_test.items() if '_unalt' in f]\n",
    "print(len(p_test), len(unalt))\n",
    "\n",
    "unalt_tmp = unalt.copy()\n",
    "manip_tmp = manip.copy()\n",
    "print(len(unalt_tmp), len(manip_tmp))\n",
    "\n",
    "t = int(0.1*1320)\n",
    "print(t)\n",
    "\n",
    "res = []\n",
    "\n",
    "c_counts = defaultdict(int)\n",
    "for _ in range(1320):\n",
    "    skip_f = set([k for (k, _) in res])\n",
    "    unalt_tmp = [(f, sorted(p, key=lambda t: t[-1], reverse=True)) for (f, p) in unalt_tmp if f not in skip_f]\n",
    "    unalt_tmp = sorted(unalt_tmp, key=lambda t: t[-1][0], reverse=True)\n",
    "    f, p = unalt_tmp[0]\n",
    "    c, _ = p[0]\n",
    "    res.append((f, c))\n",
    "    c_counts[c] += 1\n",
    "    \n",
    "    if c_counts[c] == t:\n",
    "        unalt_tmp = [(f, [(k, v) for (k, v) in p if k != c]) for (f, p) in unalt_tmp]\n",
    "        \n",
    "c_counts = defaultdict(int)\n",
    "for _ in range(1320):\n",
    "    skip_f = set([k for (k, _) in res])\n",
    "    manip_tmp = [(f, sorted(p, key=lambda t: t[-1], reverse=True)) for (f, p) in manip_tmp if f not in skip_f]\n",
    "    manip_tmp = sorted(manip_tmp, key=lambda t: t[-1][0], reverse=True)\n",
    "    f, p = manip_tmp[0]\n",
    "    c, _ = p[0]\n",
    "    res.append((f, c))\n",
    "    c_counts[c] += 1\n",
    "    \n",
    "    if c_counts[c] == t:\n",
    "        manip_tmp = [(f, [(k, v) for (k, v) in p if k != c]) for (f, p) in manip_tmp]\n",
    "        \n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sony-NEX-7              264\n",
       "LG-Nexus-5x             264\n",
       "Motorola-X              264\n",
       "Samsung-Galaxy-Note3    264\n",
       "Motorola-Nexus-6        264\n",
       "iPhone-4s               264\n",
       "HTC-1-M7                264\n",
       "Motorola-Droid-Maxx     264\n",
       "iPhone-6                264\n",
       "Samsung-Galaxy-S4       264\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([v for (k, v) in res]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, 'submit__unalt.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        if '_unalt' in fname:\n",
    "            f.write('%s,%s\\n' % (fname, c))\n",
    "        else:\n",
    "            f.write('%s,%s\\n' % (fname, 'no_class'))\n",
    "            \n",
    "            \n",
    "with open(os.path.join(out_dir, 'submit__manip.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        if '_manip' in fname:\n",
    "            f.write('%s,%s\\n' % (fname, c))\n",
    "        else:\n",
    "            f.write('%s,%s\\n' % (fname, 'no_class'))\n",
    "            \n",
    "            \n",
    "with open(os.path.join(out_dir, 'submit.csv'.lower()), 'w') as f:\n",
    "    f.write('fname,camera\\n')\n",
    "    for fname, c in res:\n",
    "        f.write('%s,%s\\n' % (fname, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

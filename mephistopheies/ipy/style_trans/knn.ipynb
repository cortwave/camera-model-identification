{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from kaggle_camera_model_id_lib.utils import ImageList, NpzFolder, NCrops, TifFolder, TifFolderExFiles\n",
    "from kaggle_camera_model_id_lib.models import VggHead, StyleVggHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = '/home/mephistopheies/storage2/data/camera-model-id/val/'\n",
    "test_path = '/home/mephistopheies/storage2/data/camera-model-id/raw/test/'\n",
    "train_path = '/home/mephistopheies/storage2/data/camera-model-id/raw/train/'\n",
    "model_path = '/home/mephistopheies/storage2/data/camera-model-id/tmp_zotac/step1/best_model.tar'\n",
    "out_dir = '/home/mephistopheies/storage2/data/camera-model-id/submit/'\n",
    "model_type = 'Vgg19Head_E_3b_bn'\n",
    "n_classes = 9\n",
    "crop_size = 256\n",
    "step = 128\n",
    "batch_size = 5\n",
    "num_workers = 1\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "crop_center_size = 1024\n",
    "\n",
    "random_crop = transforms.RandomCrop(crop_size)\n",
    "center_crop = transforms.CenterCrop(crop_center_size)\n",
    "rvf = transforms.RandomVerticalFlip()\n",
    "rhf = transforms.RandomHorizontalFlip()\n",
    "random_flip = lambda img: rvf(rhf(img))\n",
    "\n",
    "def aug_train(img):\n",
    "    if min(img.size) > crop_center_size:\n",
    "        return random_flip(random_crop(center_crop(img)))\n",
    "    return random_flip(random_crop(img))\n",
    "\n",
    "\n",
    "def jpg_compress_np(x, quality=(70, 90)):\n",
    "    x = Image.fromarray(x)\n",
    "    out = BytesIO()\n",
    "    x.save(out, format='jpeg', \n",
    "           quality=np.random.randint(quality[0], quality[1]))\n",
    "    x = Image.open(out)\n",
    "    return np.array(x)\n",
    "\n",
    "def jpg_compress_pil(x, quality=(70, 90)):\n",
    "    out = BytesIO()\n",
    "    x.save(out, format='jpeg', \n",
    "           quality=np.random.randint(quality[0], quality[1]))\n",
    "    x = Image.open(out)\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last state:\n",
      "  TLoss: 0.078632\n",
      "  TAcc:  0.9802\n",
      "  VLoss: 0.065313\n",
      "  VAcc:  0.9828\n"
     ]
    }
   ],
   "source": [
    "model_factory = {\n",
    "    'Vgg19Head_E_2b_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_3b_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='E', batch_norm=True),\n",
    "    'Vgg11Head_A_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=True),\n",
    "    'Vgg11Head_A': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=False),\n",
    "    'StyleVggHead_bn': lambda n_classes: StyleVggHead(num_classes=n_classes, load_vgg_bn=True),\n",
    "    'IEEEfcn': lambda n_classes: IEEEfcn(n_classes)\n",
    "}\n",
    "\n",
    "model = model_factory[model_type](n_classes)\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "loss_train = checkpoint['loss_train']\n",
    "acc_train = checkpoint['acc_train']\n",
    "loss_val = checkpoint['loss_val']\n",
    "acc_val = checkpoint['acc_val']\n",
    "class_to_idx = checkpoint['class_to_idx']\n",
    "idx2class = dict([(v, k) for (k, v) in class_to_idx.items()])\n",
    "print('Last state:\\n  TLoss: %0.6f\\n  TAcc:  %0.4f\\n  VLoss: %0.6f\\n  VAcc:  %0.4f' % \n",
    "    (loss_train[-1], acc_train[-1], loss_val[-1], acc_val[-1]))\n",
    "del(checkpoint)\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4197f5f032c54598adca3b777987897d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_train = ImageFolder(\n",
    "    train_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Lambda(lambda img: NCrops(np.array(center_crop(img)), crop_size=crop_size, step=step)),\n",
    "        transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "    ]))\n",
    "idx2class = dict([(v, k) for (k, v) in ds_train.class_to_idx.items()])\n",
    "batch_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(    \n",
    "    ds_train,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=1, \n",
    "    pin_memory=True)\n",
    "\n",
    "db = defaultdict(list)\n",
    "for ix_batch, (X, Y) in tqdm_notebook(enumerate(train_loader), total=int(len(ds_train.imgs)/batch_size)):\n",
    "    feats = model.vgg(Variable(X.squeeze().cuda(), volatile=True))\n",
    "    db[idx2class[Y[0]]].append(feats.view(feats.shape[0], feats.shape[1], -1).mean(dim=2).cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LG-Nexus-5x', 'Samsung-Galaxy-Note3', 'iPhone-6', 'Motorola-X', 'Sony-NEX-7', 'Motorola-Droid-Maxx', 'iPhone-4s', 'Motorola-Nexus-6', 'HTC-1-M7', 'Samsung-Galaxy-S4'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

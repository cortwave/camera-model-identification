{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import random\n",
    "import itertools as it\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from kaggle_camera_model_id_lib.utils import PechkaBot, ImageList, NpzFolder, NCrops, TifFolder, TifFolderExFiles\n",
    "from kaggle_camera_model_id_lib.models import VggHead, StyleVggHead, IEEEfcn, ResNetFC, ResNetX, FatNet1\n",
    "from kaggle_camera_model_id_lib.models import InceptionResNetV2fc, InceptionResNetV2fcSmall\n",
    "from kaggle_camera_model_id_lib.utils import jpg_compress, equalize_v_hist, hsv_convert\n",
    "from kaggle_camera_model_id_lib.utils import scale_crop_pad, gamma_correction\n",
    "from kaggle_camera_model_id_lib.utils import patch_quality_dich, n_random_crops, n_pseudorandom_crops\n",
    "from kaggle_camera_model_id_lib.models import DANet, ResNetFeatureExtractor, AvgFcClassifier, FCDiscriminator\n",
    "from kaggle_camera_model_id_lib.models import AvgClassifier\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_factory = {\n",
    "    'Vgg19Head_E_2b_bn': lambda n_classes: VggHead(num_classes=n_classes, vgg_key='E_2b', load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_3b_bn': lambda n_classes: VggHead(num_classes=n_classes, vgg_key='E_3b', load_vgg_bn=True, batch_norm=True),\n",
    "    'Vgg19Head_E_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='E', batch_norm=True),\n",
    "    'Vgg11Head_A_bn': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=True),\n",
    "    'Vgg11Head_A': lambda n_classes: VggHead(num_classes=n_classes, load_vgg_bn=True, vgg_key='A', batch_norm=False),\n",
    "    'StyleVggHead_bn': lambda n_classes: StyleVggHead(num_classes=n_classes, load_vgg_bn=True),\n",
    "    'IEEEfcn': lambda n_classes: IEEEfcn(n_classes),\n",
    "    'resnet18fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet='resnet18'),\n",
    "    'resnet18fc': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet=None),\n",
    "    'resnet18X_pretrained': lambda n_classes: ResNetX(\n",
    "        models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=n_classes, load_resnet='resnet18'),\n",
    "    'InceptionResNetV2fc_5_10_4': lambda n_classes: InceptionResNetV2fc(\n",
    "        num_classes=n_classes, nun_block35=5, num_block17=10, num_block8=4),\n",
    "    'InceptionResNetV2fcSmall_5_10': lambda n_classes: InceptionResNetV2fcSmall(\n",
    "        num_classes=n_classes, nun_block35=5, num_block17=10),\n",
    "    'resnet34fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet34'),\n",
    "    'resnet50fc_pretrained': lambda n_classes: ResNetFC(\n",
    "        models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=n_classes, load_resnet='resnet50')\n",
    "}\n",
    "\n",
    "model_factory_gan = {\n",
    "    'resnet34_fe': lambda: ResNetFeatureExtractor(models.resnet.BasicBlock, [3, 4, 6, 3], load_resnet='resnet34'),\n",
    "    'AvgFcClassifier': lambda n_classes: AvgFcClassifier(n_classes),\n",
    "    'FCDiscriminator': lambda: FCDiscriminator(),\n",
    "    'AvgClassifier512': lambda n_classes: AvgClassifier(n_classes, 512)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  acc_train_c: 0.995298\n",
      "\n",
      "  acc_train_d: 0.995153\n",
      "\n",
      "  acc_train_g: 0.998333\n",
      "\n",
      "  acc_val: 0.974519\n",
      "\n",
      "  loss_train_c: 0.014187\n",
      "\n",
      "  loss_train_d: 0.014571\n",
      "\n",
      "  loss_train_g: 0.007592\n",
      "\n",
      "  loss_val: 0.086873\n",
      "\n",
      "  time: 536.591299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/mephistopheies/storage2/data/camera-model-id/models/resnet34fc/gan/FCDiscriminator_AvgClassifier512/var2/checkpoint.tar'\n",
    "\n",
    "model_type_fe = 'resnet34_fe'\n",
    "model_type_d = 'FCDiscriminator'\n",
    "model_type_c = 'AvgClassifier512'\n",
    "\n",
    "batch_size = 15\n",
    "\n",
    "n_classes = 10\n",
    "crop_size = 256\n",
    "step = 128\n",
    "num_workers = 1\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "model = DANet(\n",
    "        model_factory_gan[model_type_fe](),\n",
    "        model_factory_gan[model_type_d](),\n",
    "        model_factory_gan[model_type_c](n_classes))\n",
    "\n",
    "\n",
    "checkpoint = torch.load(model_path)\n",
    "state = checkpoint['model']\n",
    "model.load_state_dict(state)\n",
    "class_to_idx = checkpoint['class_to_idx']\n",
    "idx2class = dict([(v, k) for (k, v) in class_to_idx.items()])\n",
    "epoch_log = checkpoint['trainin_log']\n",
    "for k, v in sorted(epoch_log[-1].items(), key=lambda t: t[0]):\n",
    "    print('  %s: %0.6f\\n' % (k, v))\n",
    "del(checkpoint)\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.Lambda(lambda img: NCrops(np.array(img), crop_size=crop_size, step=step)),\n",
    "    transforms.Lambda(lambda crops: torch.stack([normalize(to_tensor(crop)) for crop in crops]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49500\n",
      "1602\n"
     ]
    }
   ],
   "source": [
    "train_files = glob('/home/mephistopheies/storage2/data/camera-model-id/svm_data/train_img/*/*.npz')\n",
    "print(len(train_files))\n",
    "\n",
    "test_files = glob('/home/mephistopheies/storage2/data/camera-model-id/pseudo_labels/resnet34fc__256_pretrained_random_aug_kaggle_10__phase1/*/*.npz')\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d5600258084466946a5597b2a45661"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out_dir = '/home/mephistopheies/storage2/data/camera-model-id/svm_data/features/train_original/'\n",
    "\n",
    "for fname in tqdm_notebook(train_files):\n",
    "    c = fname.split('/')[-2]\n",
    "    img = np.load(fname)['data']\n",
    "    X_var = Variable(transform(img).cuda(), volatile=True)\n",
    "    f = model.feature_exctractor(X_var)\n",
    "    \n",
    "    f_mean = f.view(f.shape[0], f.shape[1], -1).mean(dim=2).mean(dim=0).cpu().data.numpy()\n",
    "    f_max = f.view(f.shape[0], f.shape[1], -1).mean(dim=2).max(dim=0)[0].cpu().data.numpy()\n",
    "    \n",
    "    cdir = os.path.join(out_dir, c)\n",
    "    if not os.path.isdir(cdir):\n",
    "        os.makedirs(cdir)\n",
    "    \n",
    "    np.savez(\n",
    "        os.path.join(cdir, os.path.basename(fname)), \n",
    "        f_mean=f_mean,\n",
    "        f_max=f_max\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9571e7fa109d4e838ebad09e39040ec6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out_dir = '/home/mephistopheies/storage2/data/camera-model-id/svm_data/features/train_pseudo/'\n",
    "\n",
    "for fname in tqdm_notebook(test_files):\n",
    "    c = fname.split('/')[-2]\n",
    "    img = np.load(fname)['data']\n",
    "    X_var = Variable(transform(img).cuda(), volatile=True)\n",
    "    f = model.feature_exctractor(X_var)\n",
    "    \n",
    "    f_mean = f.view(f.shape[0], f.shape[1], -1).mean(dim=2).mean(dim=0).cpu().data.numpy()\n",
    "    f_max = f.view(f.shape[0], f.shape[1], -1).mean(dim=2).max(dim=0)[0].cpu().data.numpy()\n",
    "    \n",
    "    cdir = os.path.join(out_dir, c)\n",
    "    if not os.path.isdir(cdir):\n",
    "        os.makedirs(cdir)\n",
    "    \n",
    "    np.savez(\n",
    "        os.path.join(cdir, os.path.basename(fname)), \n",
    "        f_mean=f_mean,\n",
    "        f_max=f_max\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfe84a44cda47198ac3623a5f489ba3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "real_test_files = glob('/home/mephistopheies/storage2/data/camera-model-id/raw/test/no_class/*.*')\n",
    "\n",
    "out_dir = '/home/mephistopheies/storage2/data/camera-model-id/svm_data/features/test/'\n",
    "\n",
    "def loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "    \n",
    "    \n",
    "for fname in tqdm_notebook(real_test_files):\n",
    "    img = np.array(loader(fname))\n",
    "    X_var = Variable(transform(img).cuda(), volatile=True)\n",
    "    f = model.feature_exctractor(X_var)\n",
    "    \n",
    "    f_mean = f.view(f.shape[0], f.shape[1], -1).mean(dim=2).mean(dim=0).cpu().data.numpy()\n",
    "    f_max = f.view(f.shape[0], f.shape[1], -1).mean(dim=2).max(dim=0)[0].cpu().data.numpy()\n",
    "    \n",
    "    np.savez(\n",
    "        os.path.join(out_dir, os.path.basename(fname)), \n",
    "        f_mean=f_mean,\n",
    "        f_max=f_max\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
